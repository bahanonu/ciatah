<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Biafra Ahanonu">
  
  <link rel="shortcut icon" href="../img/favicon.ico">
<title>One-page step-by-step</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link href="../css/extra.css" rel="stylesheet" />
  <link href="../css/pygments.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "One-page step-by-step";
    var mkdocs_page_input_path = "pipeline_detailed_all.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> CIAtah</a>
        
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">End-to-end</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../all_docs/">One-page readme</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../all_docs/">One-page readme (auto)</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Setup</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../install/">Quick Start</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../install_alt/">Install</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../example_data/">Example data</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../dependencies/">Dependencies</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Repository</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../data/">Data formats</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../notes/">Notes</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../organization/">Organization</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Processing imaging data</span></p>
                <ul class="current">
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_overview/">Overview</a>
                    </li>
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">One-page step-by-step</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#spatially-downsample-raw-movies-or-convert-to-hdf5-with-modeldownsamplerawmovies">Spatially downsample raw movies or convert to HDF5 with modelDownsampleRawMovies</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#converting-inscopix-isxd-files-to-hdf5">Converting Inscopix ISXD files to HDF5</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#check-movie-registration-before-pre-processing-with-viewmovieregistrationtest">Check movie registration before pre-processing with viewMovieRegistrationTest</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#preprocessing-calcium-imaging-movies-with-modelpreprocessmovie">Preprocessing calcium imaging movies with modelPreprocessMovie</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#saveload-preprocessing-settings">Save/load preprocessing settings</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#manual-movie-cropping-with-modelmodifymovies">Manual movie cropping with modelModifyMovies</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#extracting-cells-with-modelextractsignalsfrommovie">Extracting cells with modelExtractSignalsFromMovie</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#pca-ica-mukamel-2009">PCA-ICA (Mukamel, 2009)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#mukamel-2009">Mukamel, 2009 (µ)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#ahanonu-2022-and-of-pcsics">Ahanonu, 2022 (µ and # of PCs/ICs)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#term_tol">term_tol</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#max_iter">max_iter</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cnmf-pnevmatikakis-et-al-2016">CNMF (Pnevmatikakis et al. 2016)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cnmf-e-zhou-et-al-2018">CNMF-e (Zhou et al. 2018)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#extract-inan-et-al-2021">EXTRACT (Inan et al. 2021)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#a-description-of-extract-parameters-can-be-found-at-httpsgithubcomschnitzer-labextract-publicadvanced-aspects">A description of EXTRACT parameters can be found at https://github.com/schnitzer-lab/EXTRACT-public#advanced-aspects.</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#loading-cell-extraction-output-data-for-custom-scripts">Loading cell-extraction output data for custom scripts</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#loading-cell-extraction-output-data-with-modelvarsfromfiles">Loading cell-extraction output data with modelVarsFromFiles</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#validating-cell-extraction-with-viewcellextractiononmovie">Validating cell extraction with viewCellExtractionOnMovie</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#sorting-cell-extraction-outputs-with-computemanualsortsignals">Sorting cell extraction outputs with computeManualSortSignals</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#resources-on-manual-identification">Resources on manual identification</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ciatah-manual-sorting-gui">CIAtah manual sorting GUI</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#loading-in-prior-manually-sorted-data">Loading in prior manually sorted data</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#gui-usage-on-large-imaging-datasets">GUI usage on large imaging datasets</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cell-sorting-from-the-command-line-with-signalsorter">Cell sorting from the command line with signalSorter</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#bla-one-photon-imaging-data-signal-sorting-gui">BLA one-photon imaging data signal sorting GUI</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#mpfc-one-photon-imaging-data-signal-sorting-gui-from-example_downloadtestdatam">mPFC one-photon imaging data signal sorting GUI (from example_downloadTestData.m)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#context-menu">Context menu</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#removing-cells-not-within-brain-region-with-modelmodifyregionanalysis">Removing cells not within brain region with modelModifyRegionAnalysis</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#cross-session-cell-alignment-with-computematchobjbtwntrials">Cross-session cell alignment with computeMatchObjBtwnTrials</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#output-of-computematchobjbtwntrials">Output of computeMatchObjBtwnTrials</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#notes-on-computematchobjbtwntrials-options">Notes on computeMatchObjBtwnTrials options</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#view-cross-session-cell-alignment-with-viewmatchobjbtwnsessions">View cross-session cell alignment with viewMatchObjBtwnSessions</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#save-cross-session-cell-alignment-with-modelsavematchobjbtwntrials">Save cross-session cell alignment with modelSaveMatchObjBtwnTrials</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed/">One-page step-by-step (auto)</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_downsample_raw/">1| Downsample raw movies</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_preprocess_check/">2| Check pre-process settings</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_preprocess/">3| <u><strong>Processing imaging movies</strong></u></a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_modify_movies/">4| Movie clean-up</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_signal_extraction/">5| <u><strong>Cell extraction</strong></u></a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_signal_extraction_load/">6| Loading cell extraction outputs</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_signal_extraction_validation/">7| <u><strong>Cell extraction validation</strong></u></a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_signal_sorting_manual/">8| <u><strong>Manual cell sorting</strong></u></a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_signal_region_analysis/">9| Region-specific analysis</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_cross_session/">10| <u><strong>Cross-session alignment</strong></u></a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Spinal cord motion correction</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_detailed_spinal/">Spinal cord motion correction</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Animal tracking</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../pipeline_animal_tracking/">Animal tracking (ImageJ+MATLAB)</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">API</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../api_example_pipeline/">Custom pipelines</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_ciapkg/">ciapkg</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Help</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../help_issues/">Issues and fixes</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../blank/"><div class='subsection1'>Data</div></a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_inscopix/">Inscopix</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_nwb/">Neurodata Without Borders</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../blank/"><div class='subsection1'>Interface</div></a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_contrast/">Movie display and noise</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../blank/"><div class='subsection1'>Analysis</div></a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_analysis_methods/">Analysis methods</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_large_movie_analysis/">Large movie analysis</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_stripe_removal/">Stripe removal</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_snr/">Improving signal-to-noise</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_spatial_filtering/">Spatial filtering</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_temporal_downsampling/">Temporal downsampling</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_motion_correction/">Motion correction</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_cell_extraction/">Cell extraction</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_manual_cell_sorting/">Manual cell sorting</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_cross_session_alignment/">Cross-session alignemnt</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../help_animal_tracking/">Animal tracking</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Misc</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../acknowledgments/">Acknowledgments</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../references/">Cite</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../questions/">Questions</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../license/">License</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">CIAtah</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Processing imaging data &raquo;</li>
        
      
    
    <li>One-page step-by-step</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="detailed-ciatah-processing-pipeline">Detailed CIAtah processing pipeline<a class="headerlink" href="#detailed-ciatah-processing-pipeline" title="Permanent link">&para;</a></h1>
<p>The following detailed pipeline assumes you have started a CIAtah object using the below command:</p>
<div class="codehilite"><pre><span></span><code><span class="lineno" data-linenos="1 "></span><span class="n">obj</span> <span class="p">=</span> <span class="n">ciatah</span><span class="p">;</span>
</code></pre></div>
<p>This is the one-page version of the guide. Visit the individual pages by using the sidebar at left, can be easier to follow for some.</p>
<hr />
<p>title: Spatially downsample raw movies or convert to HDF5</p>
<hr />
<h2 id="spatially-downsample-raw-movies-or-convert-to-hdf5-with-modeldownsamplerawmovies">Spatially downsample raw movies or convert to HDF5 with <code>modelDownsampleRawMovies</code><a class="headerlink" href="#spatially-downsample-raw-movies-or-convert-to-hdf5-with-modeldownsamplerawmovies" title="Permanent link">&para;</a></h2>
<p>Users have the ability to spatially downsample raw movies, often necessary to denoise the data, save storage space, and improve runtimes of later processing steps. For most data, users can downsample 2 or 4 times in each spatial dimension while still retaining sufficient pixels per cell to facilitate cell-extraction.</p>
<p>To run, either select <code>modelDownsampleRawMovies</code> in the GUI menu or type the below command after initializing a {{ site.name }} obj.</p>
<div class="codehilite"><pre><span></span><code><span class="lineno" data-linenos="1 "></span><span class="n">obj</span><span class="p">.</span><span class="n">modelDownsampleRawMovies</span><span class="p">;</span>
</code></pre></div>
<p>This will pop-up the following screen. Users can</p>
<ul>
<li>input several folders where ISXD files are by separating each folder path with a comma (<code>Folder(s) where raw HDF5s are located</code>),</li>
</ul>
<ul>
<li>specify a common root folder to save files to (<code>Folder to save downsampled HDF5s to:</code>),</li>
</ul>
<ul>
<li>and input a root directory that contains the sub-folders with the raw data (<code>Decompression source root folder(s)</code>).</li>
</ul>
<p>The function will automatically put each file in its corresponding folder, <strong>make sure folder names are unique</strong> (this should be done anyways for data analysis reasons).</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/67715130-71b2fc00-f986-11e9-970e-9d1252c25db8.png" /></p>
<h3 id="converting-inscopix-isxd-files-to-hdf5">Converting Inscopix ISXD files to HDF5<a class="headerlink" href="#converting-inscopix-isxd-files-to-hdf5" title="Permanent link">&para;</a></h3>
<p>To convert from Inscopix ISXD file format (output by nVista v3+ and nVoke) to HDF5 run <code>modelDownsampleRawMovies</code> without changing the regular expression or make sure it looks for <code>.*.isxd</code> or similar. Users will need the latest version of the <a href="https://www.inscopix.com/nVista#Data_Analysis">Inscopix Data Processing Software</a> as these functions take advantage of their API. If {{ site.name }} cannot automatically find the API, it will ask the user to direct it to the <em>root</em> location of the Inscopix Data Processing Software (see below).</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/67715327-df5f2800-f986-11e9-9f91-eeabe7688fed.png" /></p>
<!-- ****************************************** -->
<hr />
<p>title: Check movie registration before pre-processing</p>
<hr />
<h2 id="check-movie-registration-before-pre-processing-with-viewmovieregistrationtest">Check movie registration before pre-processing with <code>viewMovieRegistrationTest</code><a class="headerlink" href="#check-movie-registration-before-pre-processing-with-viewmovieregistrationtest" title="Permanent link">&para;</a></h2>
<p>Users should spatially filter one-photon or other data with background noise (e.g. neuropil). To get a feel for how the different spatial filtering affects SNR/movie data before running the full processing pipeline, run <code>viewMovieRegistrationTest</code> module. Then select either <code>matlab divide by lowpass before registering</code> or <code>matlab bandpass before registering</code> then change <code>filterBeforeRegFreqLow</code> and <code>filterBeforeRegFreqHigh</code> settings, see below.</p>
<p>Within each folder will be a sub-folder called <code>preprocRunTest</code> inside of which is a series of sub-folders called <code>preprocRun##</code> that will contain a file called <code>settings.mat</code> that can be loaded into <code>modelPreprocessMovie</code> so the same settings that worked during the test can be used during the actual pre-processing run.</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/52497447-f3f65880-2b8a-11e9-8875-c6b408e5c011.png" /></p>
<ul>
<li>
<p>You'll get an output like the below:</p>
<ul>
<li><strong>A</strong>: The top left is without any filtering while the other 3 are with different bandpass filtering options.</li>
</ul>
<ul>
<li><strong>B</strong>: Cell ΔF/F intensity profile from the raw movie. Obtain by selecting <code>Analyze-&gt;Plot profile</code> from Fiji menu after selecting a square segment running through a cell.</li>
</ul>
<ul>
<li><strong>C</strong>: Same cell ΔF/F intensity profile from the bottom/left movie (note the y-axis is the same as above). Obtained in same manner as <strong>B</strong>.</li>
</ul>
</li>
</ul>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/59561146-695ab580-8fd1-11e9-892b-ce1f5fc7800e.png" /></p>
<!-- ****************************************** -->
<hr />
<p>title: Preprocessing calcium imaging movies</p>
<hr />
<h2 id="preprocessing-calcium-imaging-movies-with-modelpreprocessmovie">Preprocessing calcium imaging movies with <code>modelPreprocessMovie</code><a class="headerlink" href="#preprocessing-calcium-imaging-movies-with-modelpreprocessmovie" title="Permanent link">&para;</a></h2>
<p>After users instantiate an object of the <code>{{ site.name }}</code> class and enter a folder, they can start preprocessing of their calcium imaging data with <code>modelPreprocessMovie</code>.</p>
<ul>
<li>See below for a series of windows to get started, the options for motion correction, cropping unneeded regions, Δ_F/F_, and temporal downsampling were selected for use in the study associated with this repository.</li>
</ul>
<ul>
<li>There is also support for various other types of movie corrections, such as detrending a movie using linear or higher-order fits to remove the effects of photobleaching.</li>
</ul>
<ul>
<li>If users have not specified the path to Miji, a window appears asking them to select the path to Miji's <code>scripts</code> folder.</li>
</ul>
<ul>
<li>If users are using the test dataset, it is recommended that they do not use temporal downsampling.</li>
</ul>
<ul>
<li>Vertical and horizontal stripes in movies (e.g. CMOS camera artifacts) can be removed via <code>stripeRemoval</code> step. Remember to select correct <code>stripOrientationRemove</code>,<code>stripSize</code>, and <code>stripfreqLowExclude</code> options in the preprocessing options menu.</li>
</ul>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/49827992-93d86700-fd3f-11e8-9936-d7143bbec3db.png" /></p>
<p>Next the user is presented with a series of options for motion correction, image registration, and cropping.:</p>
<ul>
<li>The options highlighted in green are those that should be considered by users.</li>
</ul>
<ul>
<li>Users can over their mouse over each option to get tips on what they mean.</li>
</ul>
<ul>
<li>In particular, make sure that <code>inputDatasetName</code> is correct for HDF5 files and that <code>fileFilterRegexp</code> matches the form of the calcium imaging movie files to be analyzed.</li>
</ul>
<ul>
<li>After this, the user is asked to let the algorithm know how many frames of the movie to analyze (defaults to all frames).</li>
</ul>
<ul>
<li>Then the user is asked to select a region to use for motion correction. In general, it is best to select areas with high contrast and static markers such as blood vessels. Stay away from the edge of the movie or areas outside the brain (e.g. the edge of microendoscope GRIN lens in one-photon miniature microscope movies).</li>
</ul>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/49828665-4ceb7100-fd41-11e8-9da6-9f5a510f1c13.png" /></p>
<h3 id="saveload-preprocessing-settings">Save/load preprocessing settings<a class="headerlink" href="#saveload-preprocessing-settings" title="Permanent link">&para;</a></h3>
<p>Users can also enable saving and loading of previously selected pre-processing settings by changing the red option below.</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/70419318-10b52400-1a1a-11ea-9b43-782ac6624042.png" /></p>
<p>Settings loaded from previous run (e.g. of <code>modelPreprocessMovie</code>) or file (e.g. from <code>viewMovieRegistrationTest</code> runs) are highlighted in orange. Settings that user has just changed are still highlighted in green.</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/70418766-e6169b80-1a18-11ea-9713-f5a8301fe1c1.png" /></p>
<p>The algorithm will then run all the requested preprocessing steps and presented the user with the option of viewing a slice of the processed file. Users have now completed pre-processing.</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/49829599-b53b5200-fd43-11e8-82eb-1e94fd7950e7.png" /></p>
<!-- ****************************************** -->
<hr />
<p>title: Manual movie cropping.</p>
<hr />
<h2 id="manual-movie-cropping-with-modelmodifymovies">Manual movie cropping with <code>modelModifyMovies</code><a class="headerlink" href="#manual-movie-cropping-with-modelmodifymovies" title="Permanent link">&para;</a></h2>
<p>If users need to eliminate specific regions of their movie before running cell extraction, that option is provided. Users select a region using an ImageJ interface and select <code>done</code> when they want to move onto the next movie or start the cropping. Movies have <code>NaNs</code> or <code>0s</code> added in the cropped region rather than changing the dimensions of the movie.</p>
<p>This is generally advised for movies such as miniature microscope movies imaged through a GRIN lens probe where the outside or edge or the GRIN lens are visible. This can lead to large fluctuations that can throw off some algorithms (e.g. PCA-ICA can end up assigning many components to these "signals").</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/49829899-8f627d00-fd44-11e8-96fb-2e909b4f0d78.png" /></p>
<!-- ****************************************** -->
<hr />
<p>title: Automated cell extraction.</p>
<hr />
<h2 id="extracting-cells-with-modelextractsignalsfrommovie">Extracting cells with <code>modelExtractSignalsFromMovie</code><a class="headerlink" href="#extracting-cells-with-modelextractsignalsfrommovie" title="Permanent link">&para;</a></h2>
<p>Users can run the following cell-extraction algorithms:</p>
<div class="codehilite"><pre><span></span><code><span class="lineno" data-linenos=" 1 "></span>- &lt;a href=&#39;https://searchworks.stanford.edu/view/11513617&#39;&gt;CELLMax&lt;/a&gt;
<span class="lineno" data-linenos=" 2 "></span>
<span class="lineno" data-linenos=" 3 "></span>- &lt;a href=&#39;https://github.com/mukamel-lab/CellSort&#39;&gt;PCA-ICA&lt;/a&gt;
<span class="lineno" data-linenos=" 4 "></span>
<span class="lineno" data-linenos=" 5 "></span>- &lt;a href=&#39;https://github.com/flatironinstitute/CaImAn-MATLAB&#39; target=&#39;_blank&#39;&gt;CNMF&lt;/a&gt;
<span class="lineno" data-linenos=" 6 "></span>
<span class="lineno" data-linenos=" 7 "></span>- &lt;a href=&#39;https://github.com/zhoupc/CNMF_E&#39;&gt;CNMF-E&lt;/a&gt;
<span class="lineno" data-linenos=" 8 "></span>
<span class="lineno" data-linenos=" 9 "></span>- &lt;a href=&#39;https://github.com/schnitzer-lab/EXTRACT-public&#39; target=&#39;_blank&#39;&gt;EXTRACT&lt;/a&gt;
<span class="lineno" data-linenos="10 "></span>
<span class="lineno" data-linenos="11 "></span>- etc.
</code></pre></div>
<p>by following the below set of option screens. Details on running the new Schnitzer lab cell-extraction methods (e.g. CELLMax) will be added here after they are released.</p>
<p>We normally estimate the number of PCs and ICs on the high end, manually sort to get an estimate of the number of cells, then run PCA-ICA again with IC 1.5-3x the number of cells and PCs 1-1.5x number of ICs.</p>
<p>To run CNMF or CNMF-E, run <code>loadDependencies</code> module (e.g. <code>obj.loadDependencies</code>) after {{ site.name }} class is loaded. CVX (a CNMF dependency) will also be downloaded and <code>cvx_setup</code> run to automatically set it up.</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/49830421-fa608380-fd45-11e8-8d9a-47a3d2921111.png" /></p>
<p>The resulting output (on <em>Figure 45+</em>) at the end should look something like:</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/67053021-fe42fc00-f0f4-11e9-980c-88f463cb5043.png" /></p>
<!-- ![image](https://user-images.githubusercontent.com/5241605/51728907-c2c44700-2026-11e9-9614-1a57c3a60f5f.png) -->

<!-- ****************************************** -->

<h3 id="pca-ica-mukamel-2009">PCA-ICA (Mukamel, 2009)<a class="headerlink" href="#pca-ica-mukamel-2009" title="Permanent link">&para;</a></h3>
<p>There are several parameters for PCA-ICA that users can input, these are <code>µ</code>, <code>term_tol</code>, <code>max_iter</code>, and the number of PCs and ICs to request.</p>
<h4 id="mukamel-2009">Mukamel, 2009 (<code>µ</code>)<a class="headerlink" href="#mukamel-2009" title="Permanent link">&para;</a></h4>
<p>The original Mukamel, 2009 (<a href="https://doi.org/10.1016/j.neuron.2009.08.009">https://doi.org/10.1016/j.neuron.2009.08.009</a>) paper describing PCA-ICA gives an explanation of <code>µ</code>:</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/180803955-55367e92-d1f6-494c-a78d-a1165da1b70a.png" /></p>
<p><code>Fig. S3</code> also provides some information on the effects that varying <code>µ</code> from 0 to 1 have on cell extraction quality (we have often found lower values, e.g. 0.1, to work well in most cases):</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/180803154-be738669-b90c-4cf3-850b-71441359bb25.png" /></p>
<h4 id="ahanonu-2022-and-of-pcsics">Ahanonu, 2022 (<code>µ</code> and # of PCs/ICs)<a class="headerlink" href="#ahanonu-2022-and-of-pcsics" title="Permanent link">&para;</a></h4>
<p>We also describe <code>µ</code> in our recent calcium imaging experiments and analysis book chapter, see section <code>3.15 Extraction of Neuron Shapes, Locations, and Activity Traces</code>: <a href="https://link.springer.com/protocol/10.1007/978-1-0716-2039-7_13#Sec19">https://link.springer.com/protocol/10.1007/978-1-0716-2039-7_13#Sec19</a>. </p>
<p>Further, we make a note about choosing the number</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/180802392-b134fed1-c8ab-45b5-9ee6-814100e410ed.png" /></p>
<h4 id="term_tol">term_tol<a class="headerlink" href="#term_tol" title="Permanent link">&para;</a></h4>
<p>The <code>term_tol</code> parameter is the ICA termination tolerance, e.g. when min difference between ICA iterations is below this value, the algorithm will exit (if it has not already reached <code>max_iter</code>).</p>
<h4 id="max_iter">max_iter<a class="headerlink" href="#max_iter" title="Permanent link">&para;</a></h4>
<p>The <code>max_iter</code> parameter determines how many iterations ICA will run before terminating.</p>
<h3 id="cnmf-pnevmatikakis-et-al-2016">CNMF (Pnevmatikakis et al. 2016)<a class="headerlink" href="#cnmf-pnevmatikakis-et-al-2016" title="Permanent link">&para;</a></h3>
<p>CNMF (Constrained Nonnegative Matrix Factorization) uses a modified version of NMF to reduce crosstalk between signals and also outputs model-based traces with reduced noise. It is recommended that users compare both the model-based, smoothed traces and the more noisy dF/F traces extracted from the movie as each can be useful for different types of analyses.</p>
<p>A description of many of the parameters can be found at <a href="https://caiman.readthedocs.io/en/master/Getting_Started.html#parameters">https://caiman.readthedocs.io/en/master/Getting_Started.html#parameters</a>.</p>
<h3 id="cnmf-e-zhou-et-al-2018">CNMF-e (Zhou et al. 2018)<a class="headerlink" href="#cnmf-e-zhou-et-al-2018" title="Permanent link">&para;</a></h3>
<p>Use CNMF-e primarily on one-photon datasets or those with large background fluctuations, it will generally perform better than CNMF in those situations.</p>
<ul>
<li>An overview of the CNMF-e model can be found at <a href="https://github.com/zhoupc/CNMF_E/wiki/Model-overview">https://github.com/zhoupc/CNMF_E/wiki/Model-overview</a>.</li>
</ul>
<ul>
<li>Inscopix provides a good description of parameters at: <a href="https://github.com/inscopix/inscopix-cnmfe/blob/main/docs/parameter_tuning.md">https://github.com/inscopix/inscopix-cnmfe/blob/main/docs/parameter_tuning.md</a>.</li>
</ul>
<h3 id="extract-inan-et-al-2021">EXTRACT (Inan et al. 2021)<a class="headerlink" href="#extract-inan-et-al-2021" title="Permanent link">&para;</a></h3>
<p>EXTRACT improves signal estimation via robust estimation to reduce contamination from surrounding noise sources (be they nearby cells or background activity).</p>
<h2 id="a-description-of-extract-parameters-can-be-found-at-httpsgithubcomschnitzer-labextract-publicadvanced-aspects">A description of EXTRACT parameters can be found at <a href="https://github.com/schnitzer-lab/EXTRACT-public#advanced-aspects">https://github.com/schnitzer-lab/EXTRACT-public#advanced-aspects</a>.<a class="headerlink" href="#a-description-of-extract-parameters-can-be-found-at-httpsgithubcomschnitzer-labextract-publicadvanced-aspects" title="Permanent link">&para;</a></h2>
<p>title: Loading cell extraction data.</p>
<hr />
<h2 id="loading-cell-extraction-output-data-for-custom-scripts">Loading cell-extraction output data for custom scripts<a class="headerlink" href="#loading-cell-extraction-output-data-for-custom-scripts" title="Permanent link">&para;</a></h2>
<p>Users can load outputs from cell extraction using the below command. This will then allow users to use the images and activity traces for downstream analysis as needed.</p>
<div class="codehilite"><pre><span></span><code><span class="lineno" data-linenos="1 "></span><span class="p">[</span><span class="n">inputImages</span><span class="p">,</span><span class="n">inputSignals</span><span class="p">,</span><span class="n">infoStruct</span><span class="p">,</span><span class="n">algorithmStr</span><span class="p">,</span><span class="n">inputSignals2</span><span class="p">]</span> <span class="p">=</span> <span class="n">ciapkg</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="n">loadSignalExtraction</span><span class="p">(</span><span class="s">&#39;pathToFile&#39;</span><span class="p">);</span>
</code></pre></div>
<p>Note, the outputs correspond to the below:</p>
<ul>
<li><code>inputImages</code> - 3D or 4D matrix containing cells and their spatial information, format: [x y nCells].</li>
</ul>
<ul>
<li><code>inputSignals</code> - 2D matrix containing activity traces in [nCells nFrames] format.</li>
</ul>
<ul>
<li><code>infoStruct</code> - contains information about the file, e.g. the 'description' property that can contain information about the algorithm.</li>
</ul>
<ul>
<li><code>algorithmStr</code> - String of the algorithm name.</li>
</ul>
<ul>
<li><code>inputSignals2</code> - same as inputSignals but for secondary traces an algorithm outputs.</li>
</ul>
<!-- ****************************************** -->

<h3 id="loading-cell-extraction-output-data-with-modelvarsfromfiles">Loading cell-extraction output data with <code>modelVarsFromFiles</code><a class="headerlink" href="#loading-cell-extraction-output-data-with-modelvarsfromfiles" title="Permanent link">&para;</a></h3>
<p>In general, after running cell-extraction (<code>modelExtractSignalsFromMovie</code>) on a dataset, run the <code>modelVarsFromFiles</code> module. This allows <code>{{ site.name }}</code> to load/pre-load information about that cell-extraction run.</p>
<p>If you had to restart MATLAB or are just loading {{ site.name }} fresh but have previously run cell extraction, run this method before doing anything else with that cell-extraction data.</p>
<p>A menu will pop-up like below when <code>modelVarsFromFiles</code> is loaded, you can normally just leave the defaults as is.</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/67052600-7f00f880-f0f3-11e9-9555-96fe32b4de6d.png" /></p>
<!-- ****************************************** -->

<hr />
<p>title: Validating cell extraction data.</p>
<hr />
<h2 id="validating-cell-extraction-with-viewcellextractiononmovie">Validating cell extraction with <code>viewCellExtractionOnMovie</code><a class="headerlink" href="#validating-cell-extraction-with-viewcellextractiononmovie" title="Permanent link">&para;</a></h2>
<p>After users have run cell extraction, they should check that cells are not being missed during the process. Running the method <code>viewCellExtractionOnMovie</code> will create a movie with outlines of cell extraction outputs overlaid on the movie.</p>
<p>Below is an example, with black outlines indicating location of cell extraction outputs. If users see active cells (red flashes) that are not outlined, that indicates either exclusion or other parameters should be altered in the previous <code>modelExtractSignalsFromMovie</code> cell extraction step.</p>
<p><img alt="2014_04_01_p203_m19_check01_raw_viewCellExtractionOnMovie_ezgif-4-57913bcfdf3f_2" src="https://user-images.githubusercontent.com/5241605/59560798-50033a80-8fcc-11e9-8228-f9a3d83ca591.gif" /></p>
<p><img alt="2014_04_01_p203_m19_check01_overlay2-2" src="https://github.com/bahanonu/ciatah/assets/5241605/ee5c104a-7b4c-4fe5-b2da-63ddac8955fb" /></p>
<!-- ****************************************** -->
<hr />
<p>title: Sorting cell extraction outputs.</p>
<hr />
<h2 id="sorting-cell-extraction-outputs-with-computemanualsortsignals">Sorting cell extraction outputs with <code>computeManualSortSignals</code><a class="headerlink" href="#sorting-cell-extraction-outputs-with-computemanualsortsignals" title="Permanent link">&para;</a></h2>
<p align="center">

  <strong>{{ site.name }} cell sorting GUI</strong>

</p>

<p align="center">

  <a href="https://user-images.githubusercontent.com/5241605/100851700-64dec280-343a-11eb-974c-d6d29faf9eb2.gif">

    <img src="https://user-images.githubusercontent.com/5241605/100851700-64dec280-343a-11eb-974c-d6d29faf9eb2.gif" align="center" title="ciapkgMovie" alt="ciapkgMovie" width="100%" style="margin-left:auto;margin-right:auto;display:block;margin-bottom: 1%;">

  </a>

</p>

<p>Outputs from most common cell-extraction algorithms like PCA-ICA, CNMF, etc. contain signal sources that are not cells and thus must be manually removed from the output. The repository contains a GUI for sorting cells from not cells. GUI also contains a shortcut menu that users can access by right-clicking or selecting the top-left menu.</p>
<h3 id="resources-on-manual-identification">Resources on manual identification<a class="headerlink" href="#resources-on-manual-identification" title="Permanent link">&para;</a></h3>
<p><img alt="image" src="../img/Ahanonu_Kitch_manualSort01.png" /></p>
<p>The above figure gives an overview of the CIAtah manual sorting GUI along with examples of candidate cells that are accepted or rejected based on a variety of criteria from several cell extraction algorithms (CELLMax, PCA-ICA, and CNMF). We have discussed manual sorting previously, see the below resources:</p>
<ul>
<li><code>3.15.1 Manual Neuron Identification</code> in our miniature microscope book chapter contains a guide on manual sorting: <a href="https://link.springer.com/protocol/10.1007/978-1-0716-2039-7_13#Sec20">https://link.springer.com/protocol/10.1007/978-1-0716-2039-7_13#Sec20</a>.</li>
</ul>
<ul>
<li><code>Fig. 7: Calcium imaging analysis of nociceptive ensemble.</code> contains example accepted and rejected cells: <a href="https://link.springer.com/protocol/10.1007/978-1-0716-2039-7_13/figures/7">https://link.springer.com/protocol/10.1007/978-1-0716-2039-7_13/figures/7</a>.</li>
</ul>
<p>Below are several potential criteria to use for accepting or rejecting candidate cells output by a cell extraction algorithm:</p>
<ul>
<li>Filter shape—e.g., cell-like depending on if using one- or two-photon imaging).</li>
</ul>
<ul>
<li>The event triggered movie activity—e.g., whether it conformed to prior expectation of one-photon neuron morphology and fluorescent indicator activity. <strong>Note</strong> This criteria is critical, as some methods output candidate cells whose cell shape and activity trace look like a cell, but when the movie is checked can see that it is not a cell.</li>
</ul>
<ul>
<li>Location within the imaging field of view—e.g., not within a blood vessel.</li>
</ul>
<ul>
<li>The shape of the transient having characteristic fluorescent indicator dynamics, this will depending on the indicator being used, e.g. GCaMP will have a different expected waveform than other indicators.</li>
</ul>
<ul>
<li>Whether cell is a duplicate cell, e.g. some algorithms will "split" a cell into multiple candidate cells. This can be handled by re-running the algorithm with improved parameters, rejected the lower SNR (or otherwise poorer quality) cell, or accepting both cells then conducting a merging operation later (and re-running the cell trace extraction portion of the algorithm if that feature is available).</li>
</ul>
<h3 id="ciatah-manual-sorting-gui">CIAtah manual sorting GUI<a class="headerlink" href="#ciatah-manual-sorting-gui" title="Permanent link">&para;</a></h3>
<p>Below users can see a list of options that are given before running the code. Options highlighted in green are those that are changed from the default settings.</p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/49845107-43322f80-fd7a-11e8-96b9-3f870d4b9009.png" /></p>
<h4 id="loading-in-prior-manually-sorted-data">Loading in prior manually sorted data<a class="headerlink" href="#loading-in-prior-manually-sorted-data" title="Permanent link">&para;</a></h4>
<p>Decisions during manual sorting are stored in the <code>private/tmp</code> folder within the root CIAtah directory (find with <code>ciapkg.getDir</code>). Alternatively, previously manually sorted outputs can be re-sorted if new selection criteria are desired. When loading the <code>computeManualSortSignals</code> GUI, select one of the two options below in the <code>Use CIAtah auto classifications?</code> setting.</p>
<ul>
<li><code>Start with TEMP manually chosen classifications (e.g. backups)</code> - this option will open up a GUI into <code>private/tmp</code> and request users select a MAT-file containing the most recent decisions that were being manually sorted.</li>
</ul>
<ul>
<li><code>Start with FINISHED manually chosen classifications</code> - will automatically load already saved manual decisions located in the same folder as the cell extraction outputs.</li>
</ul>
<p><img alt="image" src="../img/manualSort_reload01.png" /></p>
<h3 id="gui-usage-on-large-imaging-datasets">GUI usage on large imaging datasets<a class="headerlink" href="#gui-usage-on-large-imaging-datasets" title="Permanent link">&para;</a></h3>
<ul>
<li>To manually sort on large movies that will not fit into RAM, select the below options (highlighted in green). This will load only chunks of the movie asynchronously into the GUI as you sort cell extraction outputs.</li>
</ul>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/59215159-5d07d000-8b6d-11e9-8dd7-0d69d5fd38b6.png" /></p>
<h3 id="cell-sorting-from-the-command-line-with-signalsorter">Cell sorting from the command line with <code>signalSorter</code><a class="headerlink" href="#cell-sorting-from-the-command-line-with-signalsorter" title="Permanent link">&para;</a></h3>
<p>Usage instructions below for <code>signalSorter</code>, e.g. if not using the <code>{{ site.name }}</code> GUI.</p>
<p><strong>Main inputs</strong></p>
<ul>
<li><code>inputImages</code> - [x y N] matrix where N = number of images, x/y are dimensions.</li>
</ul>
<ul>
<li><code>inputSignals</code> - [N frames] <em>double</em> matrix where N = number of signals (traces).</li>
</ul>
<ul>
<li><code>inputMovie</code> - [x y frames] matrix</li>
</ul>
<p><strong>Main outputs</strong></p>
<ul>
<li><code>choices</code> - [N 1] vector of 1 = cell, 0 = not a cell</li>
</ul>
<ul>
<li><code>inputImagesSorted</code> - [x y N] filtered by <code>choices</code></li>
</ul>
<ul>
<li><code>inputSignalsSorted</code> - [N frames] filtered by <code>choice</code></li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="lineno" data-linenos=" 1 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">inputMovie</span> <span class="p">=</span> <span class="n">inputMovie</span><span class="p">;</span> <span class="c">% movie associated with traces</span>
<span class="lineno" data-linenos=" 2 "></span>
<span class="lineno" data-linenos=" 3 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">valid</span> <span class="p">=</span> <span class="s">&#39;neutralStart&#39;</span><span class="p">;</span> <span class="c">% all choices start out gray or neutral to not bias user</span>
<span class="lineno" data-linenos=" 4 "></span>
<span class="lineno" data-linenos=" 5 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">cropSizeLength</span> <span class="p">=</span> <span class="mi">20</span><span class="p">;</span> <span class="c">% region, in px, around a signal source for transient cut movies (subplot 2)</span>
<span class="lineno" data-linenos=" 6 "></span>
<span class="lineno" data-linenos=" 7 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">cropSize</span> <span class="p">=</span> <span class="mi">20</span><span class="p">;</span> <span class="c">% see above</span>
<span class="lineno" data-linenos=" 8 "></span>
<span class="lineno" data-linenos=" 9 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">medianFilterTrace</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c">% whether to subtract a rolling median from trace</span>
<span class="lineno" data-linenos="10 "></span>
<span class="lineno" data-linenos="11 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">subtractMean</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c">% whether to subtract the trace mean</span>
<span class="lineno" data-linenos="12 "></span>
<span class="lineno" data-linenos="13 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">movieMin</span> <span class="p">=</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">;</span> <span class="c">% helps set contrast for subplot 2, preset movie min here or it is calculated</span>
<span class="lineno" data-linenos="14 "></span>
<span class="lineno" data-linenos="15 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">movieMax</span> <span class="p">=</span> <span class="mf">0.05</span><span class="p">;</span> <span class="c">% helps set contrast for subplot 2, preset movie max here or it is calculated</span>
<span class="lineno" data-linenos="16 "></span>
<span class="lineno" data-linenos="17 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">backgroundGood</span> <span class="p">=</span> <span class="p">[</span><span class="mi">208</span><span class="p">,</span><span class="mi">229</span><span class="p">,</span><span class="mi">180</span><span class="p">]</span><span class="o">/</span><span class="mi">255</span><span class="p">;</span>
<span class="lineno" data-linenos="18 "></span>
<span class="lineno" data-linenos="19 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">backgroundBad</span> <span class="p">=</span> <span class="p">[</span><span class="mi">244</span><span class="p">,</span><span class="mi">166</span><span class="p">,</span><span class="mi">166</span><span class="p">]</span><span class="o">/</span><span class="mi">255</span><span class="p">;</span>
<span class="lineno" data-linenos="20 "></span>
<span class="lineno" data-linenos="21 "></span><span class="n">iopts</span><span class="p">.</span><span class="n">backgroundNeutral</span> <span class="p">=</span> <span class="nb">repmat</span><span class="p">(</span><span class="mi">230</span><span class="p">,[</span><span class="mi">1</span> <span class="mi">3</span><span class="p">])</span><span class="o">/</span><span class="mi">255</span><span class="p">;</span>
<span class="lineno" data-linenos="22 "></span>
<span class="lineno" data-linenos="23 "></span><span class="p">[</span><span class="n">inputImagesSorted</span><span class="p">,</span> <span class="n">inputSignalsSorted</span><span class="p">,</span> <span class="n">choices</span><span class="p">]</span> <span class="p">=</span> <span class="n">signalSorter</span><span class="p">(</span><span class="n">inputImages</span><span class="p">,</span> <span class="n">inputSignals</span><span class="p">,</span> <span class="s">&#39;options&#39;</span><span class="p">,</span><span class="n">iopts</span><span class="p">);</span>
</code></pre></div>
<p>Examples of the interface on two different datasets:</p>
<h4 id="bla-one-photon-imaging-data-signal-sorting-gui">BLA one-photon imaging data signal sorting GUI<a class="headerlink" href="#bla-one-photon-imaging-data-signal-sorting-gui" title="Permanent link">&para;</a></h4>
<p><img alt="out-1" src="https://user-images.githubusercontent.com/5241605/34796712-3868cb3a-f60b-11e7-830e-8eec5b2c76d7.gif" /></p>
<h4 id="mpfc-one-photon-imaging-data-signal-sorting-gui-from-example_downloadtestdatam">mPFC one-photon imaging data signal sorting GUI (from <code>example_downloadTestData.m</code>)<a class="headerlink" href="#mpfc-one-photon-imaging-data-signal-sorting-gui-from-example_downloadtestdatam" title="Permanent link">&para;</a></h4>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/46322488-04c00d80-c59e-11e8-9e8a-18b3b8e4567d.png" /></p>
<h4 id="context-menu">Context menu<a class="headerlink" href="#context-menu" title="Permanent link">&para;</a></h4>
<p><a href="https://user-images.githubusercontent.com/5241605/95838435-9ec30080-0cf6-11eb-981d-fc8b5d46de7b.png" target="_blank"><img src="https://user-images.githubusercontent.com/5241605/95838435-9ec30080-0cf6-11eb-981d-fc8b5d46de7b.png" alt="drawing" width="900" height="auto" /></a></p>
<!-- ****************************************** -->
<hr />
<p>title: Removing cells not within region of interest.</p>
<hr />
<h2 id="removing-cells-not-within-brain-region-with-modelmodifyregionanalysis">Removing cells not within brain region with <code>modelModifyRegionAnalysis</code><a class="headerlink" href="#removing-cells-not-within-brain-region-with-modelmodifyregionanalysis" title="Permanent link">&para;</a></h2>
<p>If the imaging field-of-view includes cells from other brain regions, they can be removed using <code>modelModifyRegionAnalysis</code></p>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/49834696-e9b60a80-fd51-11e8-90bb-9854b7ccaeb8.png" /></p>
<!-- ****************************************** -->
<hr />
<p>title: Cross-session cell alignment.</p>
<hr />
<h2 id="cross-session-cell-alignment-with-computematchobjbtwntrials">Cross-session cell alignment with <code>computeMatchObjBtwnTrials</code><a class="headerlink" href="#cross-session-cell-alignment-with-computematchobjbtwntrials" title="Permanent link">&para;</a></h2>
<p>This step allows users to align cells across imaging sessions (e.g. those taken on different days). See the <a href="../help_cross_session_alignment/">Cross session cell alignment help page</a> for more details and notes on cross-session alignment. See below sections for notes on options.</p>
<ul>
<li>Users run <code>computeMatchObjBtwnTrials</code> to do cross-day alignment (first row in pictures below).</li>
</ul>
<ul>
<li>Users then run <code>viewMatchObjBtwnSessions</code> to get a sense for how well the alignment ran.</li>
</ul>
<ul>
<li><code>computeCellDistances</code> and <code>computeCrossDayDistancesAlignment</code> allow users to compute the within session pairwise Euclidean centroid distance for all cells and the cross-session pairwise distance for all global matched cells, respectively.</li>
</ul>
<p><img alt="image" src="https://user-images.githubusercontent.com/5241605/49835713-eec88900-fd54-11e8-8d24-f7c426802297.png" /></p>
<h3 id="output-of-computematchobjbtwntrials">Output of <code>computeMatchObjBtwnTrials</code><a class="headerlink" href="#output-of-computematchobjbtwntrials" title="Permanent link">&para;</a></h3>
<p>The output for cross-session alignment for each animal is stored in a structure within the current <code>{{ code.mainclass }}</code> object: <code>obj.globalIDStruct.ANIMAL_ID</code> where <code>ANIMAL_ID</code> is the animal identification automatically pulled from folder names (if none is found, defaults to <code>m0</code>). Users can then get the matrix that gives the session IDs from the <code>{{ code.mainclass }}</code> class:</p>
<div class="codehilite"><pre><span></span><code><span class="lineno" data-linenos=" 1 "></span><span class="c">% Grab the cross session alignment structure from the current `{{ code.mainclass }}` object. </span>
<span class="lineno" data-linenos=" 2 "></span>
<span class="lineno" data-linenos=" 3 "></span><span class="n">alignmentStruct</span> <span class="p">=</span> <span class="n">obj</span><span class="p">.</span><span class="n">globalIDStruct</span><span class="p">.</span><span class="n">ANIMAL_ID</span>
<span class="lineno" data-linenos=" 4 "></span>
<span class="lineno" data-linenos=" 5 "></span>
<span class="lineno" data-linenos=" 6 "></span>
<span class="lineno" data-linenos=" 7 "></span><span class="c">% Global IDs is a matrix of [globalID sessionID].</span>
<span class="lineno" data-linenos=" 8 "></span>
<span class="lineno" data-linenos=" 9 "></span><span class="c">% Each (globalID, sessionID) pair gives the within session ID for that particular global ID.</span>
<span class="lineno" data-linenos="10 "></span>
<span class="lineno" data-linenos="11 "></span><span class="n">globalIDs</span> <span class="p">=</span> <span class="n">alignmentStruct</span><span class="p">.</span><span class="n">globalIDs</span><span class="p">;</span>
</code></pre></div>
<p>Below is an example of what that <code>globalIDs</code> matrix looks like when visualized. Each column is an imaging session and each row is an individual global cell with the color indicating that global cell's within-session number. Any black cells indicate where no match was found for that global cell in that imaging day.</p>
<p><a href="https://user-images.githubusercontent.com/5241605/126750867-1ea1bcff-b3d1-493f-aac9-b7b7c7292796.png" target="_blank"><img src="https://user-images.githubusercontent.com/5241605/126750867-1ea1bcff-b3d1-493f-aac9-b7b7c7292796.png" alt="Global cell output" width="70%"/></a></p>
<h3 id="notes-on-computematchobjbtwntrials-options">Notes on <code>computeMatchObjBtwnTrials</code> options<a class="headerlink" href="#notes-on-computematchobjbtwntrials-options" title="Permanent link">&para;</a></h3>
<p>After starting <code>computeMatchObjBtwnTrials</code>, the below options screen will appear:   </p>
<p><a href="https://user-images.githubusercontent.com/5241605/126746771-c0486ab8-aec1-429d-b982-88f638a400a8.png" target="_blank"><img src="https://user-images.githubusercontent.com/5241605/126746771-c0486ab8-aec1-429d-b982-88f638a400a8.png" alt="Cross session options screen" width="100%"/></a></p>
<p>An explanation of each option is as follows:</p>
<ul>
<li>
<p><code>Number of rounds to register images (integer)</code></p>
<ul>
<li>This determines the number of rounds to register all the sessions to the "base" session used for alignment. Additional rounds of registration (e.g. we at times use up to 5 rounds) can often improve results especially in cases where there might be large lateral displacements across sessions.</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>Distance threshold to match cells cross-session (in pixels)</code></p>
<ul>
<li>This determine the maximum distance that the algorithm should use to match cells across sessions. Ideally this value should be <em>below</em> the within-session distance between cells to minimize false positives (e.g. matching nearby cells across sessions that are actually different cells).</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>Image binarization threshold (0 to 1, fraction each image''s max value)</code></p>
<ul>
<li>This threshold is used to remove parts of the cell filter that are not necessarily useful for cross-session alignment, such as faint dendrites or axons along with noise produced by some algorithms in their filters (e.g. as is the case with PCA-ICA).</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>Session to align to (leave blank to auto-calculate middle session to use for alignment)</code></p>
<ul>
<li>Leaving blank automatically selects the middle session, as this session is often a compromise between changes (e.g. drift in the field of view) that occurred between the 1st and last session.</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>Registration type (3 = rotation and iso scaling, 2 = rotation no iso scaling)</code></p>
<ul>
<li>This is the type of <em>Turboreg</em> registration used to align the sessions during cross-session motion correction. Avoid using iso scaling enabled (e.g. <code>3</code> or projective) unless you know in advance that you have warping in your field of view across days, else this option can lead to less optimal results compared to iso scaling disabled (e.g. <code>2</code> or affine).</li>
</ul>
<ul>
<li>See <a href="https://www.mathworks.com/help/images/matrix-representation-of-geometric-transformations.html">https://www.mathworks.com/help/images/matrix-representation-of-geometric-transformations.html</a>.</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>Run image correlation threshold? (1 = yes, 0 = no)</code></p>
<ul>
<li>This determines whether a secondary measure will be used to match cells across sessions and decreases the probability of false positives. It does this by correlating the putative matched cell to others that have been already matched to be the same cells and adds it to the "global cell" group for that cell if it passes a pre-defined threshold as below. In general this should be enabled unless you know the imaging quality varies across sessions that would lead to a distortion in cell shapes or you are using a cell-extraction algorithm that does not produce high-quality filters.</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>Image correlation type (e.g. "corr2","jaccard")</code></p>
<ul>
<li>This is the type of correlation measure, where <code>corr2</code> is <a href="https://www.mathworks.com/help/images/ref/corr2.html">2-D correlation coefficient</a> and <code>jaccard</code> is the <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard distance</a>. The <a href="https://en.wikipedia.org/wiki/Cosine_similarity">Ochiai similarity</a> is also supported.</li>
</ul>
<ul>
<li>A list of all possible measures can be found at <a href="https://www.mathworks.com/matlabcentral/fileexchange/55190-simbin-mat1-mat2-type-mask">https://www.mathworks.com/matlabcentral/fileexchange/55190-simbin-mat1-mat2-type-mask</a>. Note, some might not be valid and in general the above three should work for most users.</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>Image correlation threshold for matched cells (0 to 1)</code></p>
<ul>
<li>How high the image correlation needs to be for it to be considered a match, e.g. accept the match if it has an image correlation above this amount <em>and</em> a distance below that specified above.</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>Image correlation binarization threshold (0 to 1, fraction each image''s max value)</code></p>
<ul>
<li>This is the threshold used for calculated image correlations. <strong>Note</strong> this is different that the threshold used for cross-session cell alignment as sometimes the cross-session threshold needs to be a different value to improve alignment compared to a more relaxed threshold to improve estimation of cell shape (e.g. too high of a threshold can make all cells look similar depending on the algorithm).</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>Threshold below which registered image values set to zero</code></p>
<ul>
<li>During registration zero values can sometimes take on very small numerical values that can cause problems for downstream analysis. This threshold sets all pixels below this value to zero to correct for this. For the most part do not change this value.</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>Visually compare image correlation values and matched images (1 = yes, 0 = no)</code></p>
<ul>
<li>This will pop-up a GUI after running cross-session alignment to show matches that users can scroll through.</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><code>View full results after [viewMatchObjBtwnSessions] (1 = yes, 0 = no)</code></p>
<ul>
<li>This will pop-up several figures showing example cells matched across sessions along with graphs that show cross-session matches with each cell colored by its global identification to help determine accuracy of results. If users go to <code>obj.picsSavePath</code> and look under the folders <code>matchObjColorMap</code>, they will find AVI and picture files with outputs related to these figures.</li>
</ul>
</li>
</ul>
<!-- ### Check within-session -->

<h3 id="view-cross-session-cell-alignment-with-viewmatchobjbtwnsessions">View cross-session cell alignment with <code>viewMatchObjBtwnSessions</code><a class="headerlink" href="#view-cross-session-cell-alignment-with-viewmatchobjbtwnsessions" title="Permanent link">&para;</a></h3>
<p>To evaluate how well cross-session alignment works, <code>computeMatchObjBtwnTrials</code> will automatically run <code>viewMatchObjBtwnSessions</code> at the end, but users can also run it separately after alignment. The left are raw dorsal striatum cell maps from a single animal. The right shows after cross-session alignment; color is used to indicate a global ID cell (e.g. the same cell matched across multiple days). Thus, same color cell = same cell across sessions.</p>
<p><a href="https://cloud.githubusercontent.com/assets/5241605/25643108/9bcfccda-2f52-11e7-8514-31968752bd95.gif" target="_blank"><img src="https://cloud.githubusercontent.com/assets/5241605/25643108/9bcfccda-2f52-11e7-8514-31968752bd95.gif" alt="2017_05_02_p545_m121_p215_raw" width="auto" height="400"/></a></p>
<p><a href="https://cloud.githubusercontent.com/assets/5241605/25643473/dd7b11ce-2f54-11e7-8d84-eb98c5ef801c.gif" target="_blank"><img src="https://cloud.githubusercontent.com/assets/5241605/25643473/dd7b11ce-2f54-11e7-8d84-eb98c5ef801c.gif" alt="2017_05_02_p545_m121_p215_corrected_biafraalgorithm2" width="auto" height="400"/></a></p>
<h3 id="save-cross-session-cell-alignment-with-modelsavematchobjbtwntrials">Save cross-session cell alignment with <code>modelSaveMatchObjBtwnTrials</code><a class="headerlink" href="#save-cross-session-cell-alignment-with-modelsavematchobjbtwntrials" title="Permanent link">&para;</a></h3>
<p>Users can save out the alignment structure by running <code>modelSaveMatchObjBtwnTrials</code>. This will allow users to select a folder where <code>{{ site.name }}</code> will save a MAT-file with the alignment structure information for each animal.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../pipeline_detailed/" class="btn btn-neutral float-right" title="One-page step-by-step (auto)">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../pipeline_overview/" class="btn btn-neutral" title="Overview"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../pipeline_overview/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../pipeline_detailed/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
